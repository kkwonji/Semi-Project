{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq3UR0UMbXDP",
        "outputId": "fa909db1-fe72-4724-acd7-f5d30a953048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uNLI1GptYmz",
        "outputId": "3f95dec0-5d43-40cd-c56c-5e736540969f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1pvIqpy8sVkkUr2OgpRrXK2RrbzxuRmZ0/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "b8igqAQyemr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ],
      "metadata": {
        "id": "WxTel2ZkqqZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "ehv6Pu0isxht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install python-mecab-ko"
      ],
      "metadata": {
        "id": "5pThK4Wgy84d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U sentence-transformers"
      ],
      "metadata": {
        "id": "lGTQHFf5HqK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import nltk\n",
        "import csv\n",
        "from konlpy.tag import Hannanum, Okt\n",
        "import re\n",
        "from mecab import MeCab\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import joblib\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "tf.random.set_seed(11)"
      ],
      "metadata": {
        "id": "nT99f10Iej08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def news_predict(news, model):\n",
        "\n",
        "\n",
        "  if model == 'rf':\n",
        "\n",
        "    print(f'모델: rf')\n",
        "\n",
        "    # 텍스트 토큰화\n",
        "    okt = Okt()\n",
        "    news_title = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', news)\n",
        "    news_title = okt.morphs(news_title, norm=True, stem=True)\n",
        "\n",
        "    print(f'분류 할 제목: {news}')\n",
        "    print(f'토큰화 된 제목: {news_title}')\n",
        "\n",
        "\n",
        "    # 모델 경로\n",
        "    classifier_path = '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/classifier_model/b_okt_keras_rf_saved_model1.pkl'\n",
        "    prepro_path = '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/tokenizer/b_okt_keras.pickle'\n",
        "\n",
        "    # 학습된 임베딩 모델 불러오기\n",
        "    with open(prepro_path, 'rb') as f:\n",
        "      a_prepro_model = pickle.load(f)\n",
        "\n",
        "    # 불러온 임베딩 모델로 처리\n",
        "    a_prepro_model.fit_on_texts(news_title)\n",
        "    encoded = a_prepro_model.texts_to_sequences([news_title])\n",
        "    final_sentence = pad_sequences(encoded, maxlen=9)\n",
        "    print(f'임베딩 된 제목: {final_sentence}')\n",
        "\n",
        "    # 학습된 분류 모델 불러오기\n",
        "    classifier_model = joblib.load(classifier_path)\n",
        "\n",
        "    # 불러온 분류모델에 임베딩된 뉴스 제목 삽입\n",
        "    result = float(classifier_model.predict(final_sentence))\n",
        "\n",
        "    if(result == 1):\n",
        "      print(\"상향\")\n",
        "    else:\n",
        "      print(\"하향\")\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "  elif model == 'nb':\n",
        "\n",
        "    print(f'모델: nb')\n",
        "\n",
        "    # 텍스트 토큰화: 없음\n",
        "    print(f'분류 할 제목: {news}')\n",
        "    print(f'토큰화 된 제목: 토큰화 별도로 없음')\n",
        "\n",
        "    # 모델 경로\n",
        "    classifier_path = '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/classifier_model/nb_sbert.pkl'\n",
        "\n",
        "    # 학습된 임베딩 모델 불러오기\n",
        "    model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "    # 불러온 임베딩 모델로 처리\n",
        "    final_sentence = model.encode(news).reshape(1,-1)\n",
        "    print(f'임베딩 된 제목: 길어서 생략')\n",
        "\n",
        "    # 학습된 분류 모델 불러오기\n",
        "    loaded_model = joblib.load(classifier_path)\n",
        "\n",
        "    # 불러온 분류모델에 임베딩된 뉴스 제목 삽입\n",
        "    result = float(loaded_model.predict(final_sentence))\n",
        "\n",
        "    if(result == 1):\n",
        "      print(\"상향\")\n",
        "    else:\n",
        "      print(\"하향\")\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "  elif model == 'adaboost':\n",
        "\n",
        "    print(f'모델: adaboost')\n",
        "\n",
        "    #### ada + b_mecab_keras\n",
        "\n",
        "    # 텍스트 토큰화\n",
        "    mecab = MeCab()\n",
        "    news_title = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', news)\n",
        "    news_title = mecab.morphs(news_title)\n",
        "\n",
        "    print(f'분류 할 제목: {news}')\n",
        "    print(f'토큰화 된 제목: {news_title}')\n",
        "\n",
        "\n",
        "    # 모델 경로\n",
        "    classifier_path = '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/classifier_model/ada_b_mecab_keras.pkl'\n",
        "    prepro_path = '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/tokenizer/b_mecab_keras.pickle'\n",
        "\n",
        "    # 학습된 임베딩 모델 불러오기\n",
        "    with open(prepro_path, 'rb') as f:\n",
        "      b_prepro_model = pickle.load(f)\n",
        "\n",
        "    # 불러온 임베딩 모델로 처리\n",
        "    b_prepro_model.fit_on_texts(news_title)\n",
        "    encoded = b_prepro_model.texts_to_sequences([news_title])\n",
        "    final_sentence = pad_sequences(encoded, maxlen=9)\n",
        "    print(f'임베딩 된 제목: {final_sentence}')\n",
        "\n",
        "    # 학습된 분류 모델 불러오기\n",
        "    classifier_model = joblib.load(classifier_path)\n",
        "\n",
        "    # 불러온 분류모델에 임베딩된 뉴스 제목 삽입\n",
        "    result = float(classifier_model.predict(final_sentence))\n",
        "\n",
        "    if(result == 1):\n",
        "      print(\"상향\")\n",
        "    else:\n",
        "      print(\"하향\")\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "  elif model == 'lstm':\n",
        "\n",
        "    print(f'모델: lstm')\n",
        "\n",
        "    #### lstm + a_mecab_keras\n",
        "\n",
        "    # 텍스트 토큰화\n",
        "    mecab = MeCab()\n",
        "    news_title = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', news)\n",
        "    news_title = mecab.morphs(news_title)\n",
        "\n",
        "    print(f'분류 할 제목: {news}')\n",
        "    print(f'토큰화 된 제목: {news_title}')\n",
        "\n",
        "    # 모델 경로\n",
        "    classifier_path = '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/classifier_model/a_mecab_keras_model.h5'\n",
        "    prepro_path = '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/tokenizer/a_mecab_keras.pickle'\n",
        "\n",
        "    # 학습된 임베딩 모델 불러오기\n",
        "    with open(prepro_path, 'rb') as f:\n",
        "      c_prepro_model = pickle.load(f)\n",
        "\n",
        "    # 불러온 임베딩 모델로 처리\n",
        "    c_prepro_model.fit_on_texts(news_title)\n",
        "    encoded = c_prepro_model.texts_to_sequences([news_title])\n",
        "    final_sentence = pad_sequences(encoded, maxlen=9)\n",
        "    print(f'임베딩 된 제목: {final_sentence}')\n",
        "\n",
        "    # 학습된 분류 모델 불러오기\n",
        "    classifier_model = tf.keras.models.load_model(classifier_path)\n",
        "\n",
        "    # 불러온 분류모델에 임베딩된 뉴스 제목 삽입\n",
        "    result = float(classifier_model.predict(final_sentence))\n",
        "\n",
        "    if(result == 1):\n",
        "      print(\"상향\")\n",
        "    else:\n",
        "      print(\"하향\")\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "  elif model == 'bert':\n",
        "\n",
        "    print(f'모델: bert')\n",
        "\n",
        "    #### bert + hannanum\n",
        "\n",
        "    # 텍스트 토큰화\n",
        "    # '제목' 형태소 분석\n",
        "\n",
        "    hannanum = Hannanum()\n",
        "    morphs_hannanum = []\n",
        "\n",
        "    # for news_title in data[title]: #전체 input시 반복문 사용\n",
        "    morphs_hannanum.append(hannanum.morphs(news))\n",
        "\n",
        "\n",
        "    ###################################################\n",
        "    # 불용어 리스트 생성\n",
        "    f = open('stop_words.txt', encoding = 'utf8')\n",
        "    reader = csv.reader(f)\n",
        "    stopwords = []\n",
        "    for i in reader:\n",
        "        stopwords.append(i)\n",
        "    f.close()\n",
        "\n",
        "    # 2차원으로 반환되므로 1차원리스트로 변환\n",
        "    stopword_list = np.concatenate(stopwords).tolist()\n",
        "    ##################################################\n",
        "    # 불용어 함수\n",
        "    def remove_stopwords(morphs):\n",
        "        clean_word = []\n",
        "        clean_list = []\n",
        "        for word in morphs:\n",
        "            #print('불용어 제거 문장\\n',word)\n",
        "            for token in word:\n",
        "                if token not in stopword_list:\n",
        "                    clean_word.append(token)\n",
        "\n",
        "            clean_list.append(clean_word)\n",
        "            clean_word = []\n",
        "        return clean_list\n",
        "    # 함수 사용 -> 불용어 제거 hannanum\n",
        "    clean_hannanum = remove_stopwords(morphs_hannanum)\n",
        "    #################################################\n",
        "    # 문장화 함수\n",
        "    def make_sentence(clean_list):\n",
        "        sentence_list = []\n",
        "        for row in clean_list:\n",
        "            sentence_list.append(' '.join(row))\n",
        "        return sentence_list\n",
        "    # 함수 사용 -> 문장화\n",
        "    news_title = make_sentence(clean_hannanum)\n",
        "\n",
        "    print(f'분류 할 제목: {news}')\n",
        "    print(f'토큰화 된 제목: {news_title}')\n",
        "\n",
        "\n",
        "    # 모델 불러오기\n",
        "    save_path = '/content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/classifier_model/ c_hannanum_bert_model'\n",
        "\n",
        "    # 학습된 모델 불러오기\n",
        "    loaded_model = TFBertForSequenceClassification.from_pretrained(save_path)\n",
        "\n",
        "    # input news_title 전처리 (2) bert형식으로 변환 함수\n",
        "    max_len = 9\n",
        "    klue_tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
        "    def klue_bert_tokenizer(sentence, max_len):\n",
        "      encoded_dict = klue_tokenizer.encode_plus(\n",
        "          text=sentence,\n",
        "          add_special_tokens=True,\n",
        "          max_length=max_len,\n",
        "          pad_to_max_length=True,\n",
        "          return_attention_mask=True\n",
        "      )\n",
        "      input_id = encoded_dict['input_ids']\n",
        "      attention_mask = encoded_dict['attention_mask']\n",
        "      token_type_id = encoded_dict['token_type_ids']\n",
        "      return input_id, attention_mask, token_type_id\n",
        "\n",
        "    print(f'임베딩 된 제목: 없음 ')\n",
        "    # input news_title 전처리 (3) bert형식으로 변환 후: final_sentence -> input_id, attention_mask, token_type_id\n",
        "    input_id, attention_mask, token_type_id = klue_bert_tokenizer(news_title, max_len)\n",
        "\n",
        "    input_id = np.array(input_id, dtype=int).reshape(1, -1)\n",
        "    attention_mask = np.array(attention_mask, dtype=int).reshape(1, -1)\n",
        "    token_type_id = np.array(token_type_id, dtype=int).reshape(1, -1)\n",
        "\n",
        "    # input news_title 전처리 (4) 결과 도출\n",
        "    predictions = loaded_model.predict([input_id, attention_mask, token_type_id])\n",
        "\n",
        "    result = float(np.argmax(predictions.logits[0]))\n",
        "\n",
        "    if(result == 1):\n",
        "      print(\"상향\")\n",
        "    else:\n",
        "      print(\"하향\")"
      ],
      "metadata": {
        "id": "g4VqJ8flr0Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aaa = '코스피, 2600선 안착...기관 매수 유입에 상승세 지속'"
      ],
      "metadata": {
        "id": "G-TE3NNPwebq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbb = '삼성전자·SK하이닉스 목표주가 줄상향'"
      ],
      "metadata": {
        "id": "NskL2KpvuX-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbbb = '코스피, 2600선 안착...기관 매수 유입에 상승세 지속'"
      ],
      "metadata": {
        "id": "pfgejtScvf2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "민준 = '풍부할수록 좋다…TV 점점 얇아지더니 인기 폭발한 가전'"
      ],
      "metadata": {
        "id": "AdGZrdEyvuBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "하은 = '뚝뚝 떨어진 D램 가격… 삼성전자·SK하이닉스, 2분기도 어둡다'"
      ],
      "metadata": {
        "id": "VHNkrSlkv_Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_predict(민준, model='rf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTJGW1Qqs4hy",
        "outputId": "14c02f72-ccc5-4ba1-8f97-3641e3d7d93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델: rf\n",
            "분류 할 제목: 풍부할수록 좋다…TV 점점 얇아지더니 인기 폭발한 가전\n",
            "토큰화 된 제목: ['풍부하다', '좋다', '점점', '얇다', '인기', '폭발', '한', '가전']\n",
            "임베딩 된 제목: [[  0 493  39 162 494 495 496 497 498]]\n",
            "하향\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_predict(민준, model='nb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxBpAjx3s4bO",
        "outputId": "526a2387-07d3-497e-bcb3-37d5c05624e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델: nb\n",
            "분류 할 제목: 풍부할수록 좋다…TV 점점 얇아지더니 인기 폭발한 가전\n",
            "토큰화 된 제목: 토큰화 별도로 없음\n",
            "임베딩 된 제목: 길어서 생략\n",
            "상향\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_predict(민준, model='adaboost')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVQj7rSxs4T3",
        "outputId": "02178365-10d1-4e45-f5b8-e9bcb3efd215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델: adaboost\n",
            "분류 할 제목: 풍부할수록 좋다…TV 점점 얇아지더니 인기 폭발한 가전\n",
            "토큰화 된 제목: ['풍부', '할수록', '좋', '다', '점점', '얇', '아', '지', '더니', '인기', '폭발', '한', '가전']\n",
            "임베딩 된 제목: [[147 443 444 445 446 447 448 449 450]]\n",
            "하향\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_predict(민준, model='lstm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUNNqmG3s4K0",
        "outputId": "f630f8c9-07f8-4771-83f1-7ae0d03d42db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델: lstm\n",
            "분류 할 제목: 풍부할수록 좋다…TV 점점 얇아지더니 인기 폭발한 가전\n",
            "토큰화 된 제목: ['풍부', '할수록', '좋', '다', '점점', '얇', '아', '지', '더니', '인기', '폭발', '한', '가전']\n",
            "임베딩 된 제목: [[126 374 375 376 377 378 379 380 381]]\n",
            "1/1 [==============================] - 1s 502ms/step\n",
            "하향\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_predict(민준, model='bert')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iOMlJm2s4Dl",
        "outputId": "6f5dae1f-af6f-4329-f127-9baa19e0a137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델: bert\n",
            "분류 할 제목: 풍부할수록 좋다…TV 점점 얇아지더니 인기 폭발한 가전\n",
            "토큰화 된 제목: ['풍부 좋다…TV 점점 얇 지 더니 인기 폭발 가전']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at /content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/classifier_model/ c_hannanum_bert_model were not used when initializing TFBertForSequenceClassification: ['dropout_417']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/KDT/비정형텍스트분석/KDT_1차프로젝트/최종 제출 코드/최종 뉴스 분류 모델/classifier_model/ c_hannanum_bert_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 된 제목: 없음 \n",
            "1/1 [==============================] - 5s 5s/step\n",
            "하향\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M03Ja3n21MQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}